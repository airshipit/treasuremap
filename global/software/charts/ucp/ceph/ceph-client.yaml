---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: ucp-ceph-client-global
  layeringDefinition:
    abstract: true
    layer: global
  storagePolicy: cleartext
  labels:
    name: ucp-ceph-client-global
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.ucp.ceph-client
      dest:
        path: .source

    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.ceph.ceph-client
      dest:
        path: .values.images.tags

    # IP addresses
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .storage.ceph.public_cidr
      dest:
        path: .values.network.public
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .storage.ceph.cluster_cidr
      dest:
        path: .values.network.cluster

    # Endpoints
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: ucp_endpoints
        path: .ceph.ceph_mon
      dest:
        path: .values.endpoints.ceph_mon
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: ucp_endpoints
        path: .ceph.ceph_mgr
      dest:
        path: .values.endpoints.ceph_mgr

    # Secrets
    - dest:
        path: .values.conf.ceph.global.fsid
      src:
        schema: deckhand/Passphrase/v1
        name: ceph_fsid
        path: .

data:
  chart_name: ucp-ceph-client
  release: ucp-ceph-client
  namespace: ceph
  protected:
    continue_processing: false
  wait:
    timeout: 900
    labels:
      release_group: clcp-ucp-ceph-client
    resources:
      - type: deployment
        min_ready: 1
      - type: job
    native:
      enabled: false
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: clcp-ucp-ceph-client
        - type: pod
          labels:
            release_group: clcp-ucp-ceph-client
            component: test
  test:
    enabled: true
  values:
    labels:
      job:
        node_selector_key: ucp-control-plane
        node_selector_value: enabled
      mds:
        node_selector_key: ceph-mds
        node_selector_value: enabled
      mgr:
        node_selector_key: ceph-mgr
        node_selector_value: enabled
    endpoints:
      ceph_mon:
        namespace: ceph
    deployment:
      ceph: true
    bootstrap:
      enabled: true
    ceph_mgr_enabled_modules:
      - restful
      - status
      - prometheus
      - balancer
      - iostat
    pod:
      mandatory_access_control:
        type: apparmor
        ceph-checkdns:
          ceph-checkdns: runtime/default
          init: runtime/default
        ceph-mds:
          ceph-mds: runtime/default
          ceph-init-dirs: runtime/default
        ceph-mgr:
          ceph-mgr: runtime/default
          ceph-init-dirs: runtime/default
        ceph-rbd-pool:
          ceph-rbd-pool: runtime/default
          init: runtime/default
        ceph-client-bootstrap:
          ceph-client-bootstrap: runtime/default
          init: runtime/default
        ceph-client-test:
          ceph-cluster-helm-test: runtime/default
          init: runtime/default
      replicas:
        mds: 1
        mgr: 1
      resources:
        enabled: true
        mds:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "2000m"
        mgr:
          requests:
            memory: "2Gi"
            cpu: "2000m"
          limits:
            memory: "8Gi"
            cpu: "4000m"
        checkdns:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "1Gi"
            cpu: "1000m"
        jobs:
          bootstrap:
            requests:
              memory: "512Mi"
              cpu: "500m"
            limits:
              memory: "1Gi"
              cpu: "1000m"
          image_repo_sync:
            requests:
              memory: "512Mi"
              cpu: "500m"
            limits:
              memory: "1Gi"
              cpu: "1000m"
          rbd_pool:
            requests:
              memory: "512Mi"
              cpu: "500m"
            limits:
              memory: "1Gi"
              cpu: "1000m"
          tests:
            requests:
              memory: "512Mi"
              cpu: "500m"
            limits:
              memory: "1Gi"
              cpu: "1000m"
    jobs:
      pool_checkPGs:
        # Run once a month at midnight of the first day of the month
        cron: "0 0 1 * *"
        history:
          # Number of successful job to keep
          successJob: 1
          # Number of failed job to keep
          failJob: 1
        concurrency:
          # Skip new job if previous job still active
          execPolicy: Forbid
        startingDeadlineSecs: 60
      rbd_pool:
        restartPolicy: Never
    conf:
      features:
        pg_autoscaler: false
        cluster_flags:
          set: "noup"
      pool:
        spec:
          # Health metrics pool
          - name: device_health_metrics
            application: mgr_devicehealth
            replication: 3
            percent_total_data: 5
          # RBD pool
          - name: rbd
            application: rbd
            replication: 3
            percent_total_data: 15
          # CephFS pools
          - name: cephfs_metadata
            application: cephfs
            replication: 3
            percent_total_data: 1
          - name: cephfs_data
            application: cephfs
            replication: 3
            percent_total_data: 2.5
          # RadosGW pools
          - name: .rgw.root
            application: rgw
            replication: 3
            percent_total_data: 0.1
          - name: default.rgw.control
            application: rgw
            replication: 3
            percent_total_data: 0.1
          - name: default.rgw.log
            application: rgw
            replication: 3
            percent_total_data: 5
          - name: default.rgw.intent-log
            application: rgw
            replication: 3
            percent_total_data: 0.1
          - name: default.rgw.meta
            application: rgw
            replication: 3
            percent_total_data: 0.1
          - name: default.rgw.usage
            application: rgw
            replication: 3
            percent_total_data: 0.1
          - name: default.rgw.users.uid
            application: rgw
            replication: 3
            percent_total_data: 0.1
          - name: default.rgw.buckets.non-ec
            application: rgw
            replication: 3
            percent_total_data: 0.1
          - name: default.rgw.buckets.index
            application: rgw
            replication: 3
            percent_total_data: 3
          - name: default.rgw.buckets.data
            application: rgw
            replication: 3
            percent_total_data: 34.8
        # NOTE(alanmeadowS) spport 4.x 16.04 kernels (non-HWE)
        crush:
          tunables: 'hammer'

        # NOTE(alanmeadows): This is required ATM for bootstrapping a Ceph
        # cluster with only one OSD.  Depending on OSD targeting & site
        # configuration this can be changed.
        target:
          osd: 1
          pg_per_osd: 100
          protected: true
          # NOTE: 'quota' represents a scaler for effective capacity of cluster
          # as a percent value. Setting it to anything more than 100 will mean
          # that if all pools completely use their quotas, total data stored is
          # more than capacity. For example quota at 10000 would ensure that
          # EVERY pool can exceed cluster capacity. Set to 85 in order to maintain
          # healthy state and allow data move in case of failures.
          quota: 85
        default:
          # NOTE(alanmeadows): This is required ATM for bootstrapping a Ceph
          # cluster with only one OSD.  Depending on OSD targeting & site
          # configuration this can be changed.
          crush_rule: same_host

      ceph:
        global:
          # NOTE(mb874d): This is required ATM for bootstrapping a Ceph
          # cluster with only one OSD.  Depending on OSD targeting & site
          # configuration this can be changed.
          osd_pool_default_size: 1
          mon_pg_warn_min_per_osd: 0
          mon_pg_warn_max_object_skew: 0

    manifests:
      cronjob_defragosds: false
  dependencies:
    - ceph-htk
...
